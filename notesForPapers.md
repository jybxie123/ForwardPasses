## Fine-Tuning Language Models with Just Forward Passes

1.Abstract:

This model has 12 times less memory than classic bachpropagation method.

The same memory usage as inference. In fine tune. As a result, the training time is much longer than the basic method.


2.
